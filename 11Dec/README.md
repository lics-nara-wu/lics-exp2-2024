# 第10回 2024-12-11

## はじめに
須藤の担当回では日本語の自然言語処理に関わるプログラムの作成を通じて
1. 機械学習によるデータ分類（パターン認識）の基礎
2. [scikit-learn](https://github.com/scikit-learn/scikit-learn)を使った機械学習の基礎
3. 機械学習による日本語文書分類
4. 機械学習による日本語の分かち書き
を学びます。

第10回はこのうち 3. までを行います。

## 実験環境
- G棟のLinuxを前提とします
- 実装言語はG棟のLinuxで標準利用可能な Python を基本とします
  - 自主的に他の言語で実装することは妨げません（須藤がフォローできるかどうかは分かりませんが...）
  - ただし、データの前処理に係る部分はできるだけ提供している Python スクリプトを利用してください

※ このPythonはバージョンが古いので、Pythonの最近の機能やライブラリがいろいろ使えません。この環境で動くような仕組みで説明しますので、今どきの環境ではそのまま動かないかもしれませんがご容赦ください。

## Pythonのリファレンス
- [Python 3.6ドキュメント](https://docs.python.org/ja/3.6/)
- [Python 3.6標準ライブラリリファレンス](https://docs.python.org/ja/3.6/library/index.html)
- [本実験で使うPythonのライブラリ等の使い方説明](https://github.com/lics-nara-wu/lics-exp2-2024/edit/main/README_python.md)

## 当日の実験の流れ
以下のような流れで進めます。自分で分かるという人はどんどん先に進めていただいてかまいません。

### 1コマ目
1. 実験の目的や内容の説明
2. Pythonの環境設定
3. 今回取り組む課題についての技術的説明
4. 今回の実験用データの前処理

### 2コマ目
5. 文書分類の学習用・推論用プログラムの作成
6. 分類結果の評価用プログラムの作成
7. 課題提出（時間内に終わらなければ提出期限までに提出すればOK）

## 1. 実験の目的や内容の説明
スライドを使って説明します。スライドはLMSで共有します。

## 2. Pythonの環境設定
### 2.1 作業ディレクトリの作成
まずは今回の実験で利用する作業ディレクトリを作成してください。

名前は任意ですが、仮に `${HOME}/exp2_2024_nlp` として、以下これを `${EXPDIR}` と表すことにします。
コマンドライン上で変数として `${EXPDIR}` を定義しておけば、以下はこの名前を使ってアクセスできます。
（ログアウトすると変数はリセットされてしまうので、再度使う場合は変数定義をやり直してください）
```
mkdir -p ${HOME}/exp2_2024_nlp
EXPDIR=${HOME}/exp2_2024_nlp
ls -l ${EXPDIR}
```

### 2.2 作業ディレクトリでのPython仮想環境の作成
今回の実験ではPythonのライブラリをインストールして使いますが、システムに勝手にインストールすることはできないので、自分のディレクトリ内にインストールできるような仮想環境を作成します。
仮想環境の作成にはPython標準の `venv` というライブラリを利用します。
```
cd ${EXPDIR}
python3 -m venv --prompt exp2_2024_nlp .venv
```
これで `${EXPDIR}/.venv` というディレクトリに `exp_2024_nlp` という名前の仮想環境が作成されます。

仮想環境は作成するだけでは不十分で、仮想環境を有効化 (activate) しなければなりません。
この仮想環境を利用する際には必ず以下のコマンドを実行してください。
```
source ${EXPDIR}/.venv/bin/activate
```
`source`はシェル (bash) で設定を読み込むためのコマンドで、これを実行することにより仮想環境で利用するファイルパスなどが設定されます。
設定が読み込まれると、手元のコマンドラインのプロンプト（入力欄）が以下のように変わり、先ほど作成した `exp_2024_nlp` という仮想環境が有効になっていることが分かります。
```
(exp2_2024_nlp) [sudoh@remote01 exp2_2024_nlp]$
```

### 2.3 今回の実験で利用するライブラリのインストール
#### scikit-learn
機械学習用のライブラリ `scikit-learn` をインストールします。
```
pip3 install scikit-learn
```

#### mecab
日本語の分かち書きを行うためのライブラリ `mecab-python3` をインストールします。環境の都合で特定のバージョンでないとインストールできないので、バージョンを指定します。
```
pip3 install mecab-python3==1.0.4
```

### unidic-lite
日本語の分かち書きで利用する辞書ライブラリの `unidic-lite` をインストールします。
```
pip3 install unidic-lite
```

## 3. 今回取り組む課題についての技術的説明
スライドを使って説明します。スライドはLMSで共有します。

## 4. 今回の実験用データの前処理
今回の実験で利用する文書分類用のデータとして[WRIME: 主観と客観の感情分析データセット](https://github.com/ids-cv/wrime)を利用します。
ライセンスの関係で前処理済みのデータの再配布ができないので、公開されているデータのコピーに対して皆さんのお手元で前処理を実行して利用する必要があります。

### 4.1 前処理スクリプトの作成
まず、[前処理用のPythonスクリプト](https://github.com/lics-nara-wu/lics-exp2-2024/blob/main/11Dec/scripts/extract_wrime_v2_data.py)のコピーを作成します。
以下のいずれかの方法で行ってみてください。
- GitHubの画面を見ながら写経する
- GitHubの画面で Copy raw file のボタンをクリックしてクリップボードにコピーし、何かのエディタで開いて貼り付ける
- 須藤のディレクトリにある `/export/home/ics/sudoh/Project/Exp2/2024/scripts/extract_wrime_v2_data.py` をコピーする

このファイルは `${EXPDIR}/scripts/extract_wrime_v2_data.py` として保存します。

### 4.2 前処理済みデータ格納用ディレクトリの作成
前処理済みのデータを格納するためのディレクトリを作成します。
```
mkdir -p ${EXPDIR}/data
```

### 4.3 前処理スクリプトの実行
以下のように前処理スクリプトを実行し、WRIME v2のデータの必要な箇所だけを抜き取り、JSON形式で格納したファイルを作成します。
```
python3 ${EXPDIR}/scripts/extract_wrime_v2_data.py -t ${EXPDIR}/data/wrime_v2_sentiment.tok.json
```

実行の結果、`${EXPDIR}/data/wrime_v2_sentiment.tok.json` というファイルが生成されるはずです。

この前処理スクリプトでは、必要のないデータを捨て、文と、その文を書いた人の感情極性を表す数値 (2, 1, 0, -1, -2) のみを残すようにしています。

### 4.4 分かち書きを行わないデータの作成
また、比較のため分かち書きを行っていない形式のファイルも作成しておきます。
（余力のある人はこのデータを使った実験にもトライしてみてください）
```
python3 ${EXPDIR}/scripts/extract_wrime_v2_data.py ${EXPDIR}/data/wrime_v2_sentiment.json
```

## 5. 文書分類の学習用・推論用プログラムの作成
今回の課題は、WRIME ver.2 のデータを用い、文の極性判定をする分類モデルを作成し、評価することです。


### 5.1 学習用プログラムの作成
前処理をしたJSON形式のデータを入力とし、学習した文書分類モデルを出力するプログラムを作成してください。
- 入力データファイル名の指定
- 出力モデルファイル名の指定
- 入力データの読み込み（今回のポイント）
- 入力特徴量の抽出
- scikit-learnのデータ形式への変換
- モデルの学習
- モデルの保存

[プログラムのテンプレート](https://github.com/lics-nara-wu/lics-exp2-2024/blob/main/11Dec/scripts/wrime2-classify-train.py)を用意していますので参考にしてください。


### 5.2 推論用プログラムの作成
前処理をしたJSON形式のデータと、学習済みの文書分類モデルを入力とし、文書分類の結果を出力するプログラムを作成してください。
- 入力データファイル名の指定
- 入力ファイル名の指定
- 入力データの読み込み
- 入力特徴量の抽出
- scikit-learnのデータ形式への変換
- 分類結果の推論
- 分類結果の出力

[プログラムのテンプレート](https://github.com/lics-nara-wu/lics-exp2-2024/blob/main/11Dec/scripts/wrime2-classify-test.py)を用意していますので参考にしてください。

## 6. 分類結果の評価用プログラムの作成
前処理をしたJSON形式のデータと、文書分類の結果を入力とし、文書分類の精度を評価して出力するプログラムを作成してください。
- 入力データファイル名の指定
- 入力分類結果ファイル名の指定
- 入力データ・入力分類結果の読み込み
- 評価値の計算
- 評価値の出力

[プログラムのテンプレート](https://github.com/lics-nara-wu/lics-exp2-2024/blob/main/11Dec/scripts/wrime2-classify-evaluate.py)を用意していますので参考にしてください。


## 7. 課題提出（時間内に終わらなければ提出期限までに提出すればOK）
LMSの「課題（第10回、自然言語処理1）」のところに
- 作成したプログラム
- プログラムの使い方（どのプログラムにどのような引数／オプションを与えて実行するかを最低限記載）を書いたテキストファイル

[!IMPORTANT]
提出期限は **2024-12-18 (水) 23:59 (日本標準時)** です。
提出期限後の提出も受け付けますが、減点対象です。

## 8. 発展（時間がある人はチャレンジしてみてください）
### 8.1 特徴量抽出の設定変更
今回テンプレートで利用している特徴量抽出で利用している `CountVectorizer` にはいくつかオプションで振る舞いを変更できます。例えば
- 単語一個一個だけでなく、連続する単語（二個組とか三個組とか）を特徴量とする (`ngram_range`)
- 一文の中でのある単語の個数を特徴量とするのではなく、ある単語があるかないかの二値の特徴量とする (`binary`)
- 学習データの中であまり出現回数が小さい特徴量を利用しないようにする (`min_df`)

などが使えます。余裕があればこれらを変更するとどう性能が変わるか、検証してみてください。

参考：[ドキュメント](https://scikit-learn.org/0.24/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)

### 8.2 単語区切りから文字区切りへの変更
今回は事前に分かち書きされた日本語文を使っていますが、単語での分かち書きをせずに文字の単位で処理することもできます。

[4.4 分かち書きを行わないデータの作成](#4.4-分かち書きを行わないデータの作成)で作成したデータを用い、
`CountVectorizer`に渡す文（テンプレートでは `model_pipeline` に渡す X_*_str）を、
すべての文字の間に空白を入れたものにすれば文字単位で特徴量が作成されます。

### 8.3 LinearSVCの学習設定の変更
今回文の分類に使っている LinearSVC にはいろいろな設定項目があります。例えば
- 正則化パラメータ：`C`が大きいほど正則化が弱く（過学習を避けるが、学習データでの正解率が下がる）、`C`が小さいほど正則化が強い（学習データでの正解率が上がるが、過学習しやすい）
- 学習の繰り返し回数：最大 `max_iter` 回の繰り返し計算を行う
- データ不均衡の調整：`class_weight`の指定により分類されるクラス毎のデータ量の偏りを補正する

参考：[ドキュメント](https://scikit-learn.org/0.24/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)

### 8.4 自分で入力した文に対する
