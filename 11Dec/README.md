# 第10回 2024-12-11

## はじめに
須藤の担当回では日本語の自然言語処理に関わるプログラムの作成を通じて
1. 機械学習によるデータ分類（パターン認識）の基礎
2. [scikit-learn](https://github.com/scikit-learn/scikit-learn)を使った機械学習の基礎
3. 機械学習による日本語文書分類
4. 機械学習による日本語の分かち書き
を学びます。
第10回はこのうち 3. までを行います。

## 実験環境
- G棟のLinuxを前提とします
- 実装言語はG棟のLinuxで標準利用可能な Python を基本とします
  - 自主的に他の言語で実装することは妨げません（須藤がフォローできるかどうかは分かりませんが...）
  - ただし、データの前処理に係る部分はできるだけ提供している Python スクリプトを利用してください

※ このPythonはバージョンが古いので、Pythonの最近の機能やライブラリがいろいろ使えません。
　この環境で動くような仕組みで説明しますので、今どきの環境ではそのまま動かないかもしれませんがご容赦ください。

## 当日の実験の流れ
以下のような流れで進めます。自分で分かるという人はどんどん先に進めていただいてかまいません。

### 1コマ目
1. 実験の目的や内容の説明
2. Pythonの環境設定
3. 今回取り組む課題についての技術的説明
4. 今回の実験用データの前処理

### 2コマ目
5. 文書分類の学習用・推論用プログラムの作成
6. 分類結果の評価用プログラムの作成
7. 課題提出（時間内に終わらなければ提出期限までに提出すればOK）

## 1. 実験の目的や内容の説明
スライドを使って説明します。スライドはLMSで共有します。

## 2. Pythonの環境設定
### 2.1 作業ディレクトリの作成
まずは今回の実験で利用する作業ディレクトリを作成してください。

名前は任意ですが、仮に `${HOME}/exp2_2024_nlp` として、以下これを `${EXPDIR}` と表すことにします。
コマンドライン上で変数として `${EXPDIR}` を定義しておけば、以下はこの名前を使ってアクセスできます。
（ログアウトすると変数はリセットされてしまうので、再度使う場合は変数定義をやり直してください）
```
mkdir -p ${HOME}/exp2_2024_nlp
EXPDIR=${HOME}/exp2_2024_nlp
ls -l ${EXPDIR}
```

### 2.2 作業ディレクトリでのPython仮想環境の作成
今回の実験ではPythonのライブラリをインストールして使いますが、システムに勝手にインストールすることはできないので、自分のディレクトリ内にインストールできるような仮想環境を作成します。
仮想環境の作成にはPython標準の `venv` というライブラリを利用します。
```
cd ${EXPDIR}
python3 -m venv --prompt exp2_2024_nlp .venv
```
これで `${EXPDIR}/.venv` というディレクトリに `exp_2024_nlp` という名前の仮想環境が作成されます。

仮想環境は作成するだけでは不十分で、仮想環境を有効化 (activate) しなければなりません。
この仮想環境を利用する際には必ず以下のコマンドを実行してください。
```
source ${EXPDIR}/.venv/bin/activate
```
`source`はシェル (bash) で設定を読み込むためのコマンドで、これを実行することにより仮想環境で利用するファイルパスなどが設定されます。
設定が読み込まれると、手元のコマンドラインのプロンプト（入力欄）が以下のように変わり、先ほど作成した `exp_2024_nlp` という仮想環境が有効になっていることが分かります。
```
(exp2_2024_nlp) [sudoh@remote01 exp2_2024_nlp]$
```

### 2.3 今回の実験で利用するライブラリのインストール
#### scikit-learn
機械学習用のライブラリ `scikit-learn` をインストールします。
```
pip3 install scikit-learn
```

#### mecab
日本語の分かち書きを行うためのライブラリ `mecab-python3` をインストールします。環境の都合で特定のバージョンでないとインストールできないので、バージョンを指定します。
```
pip3 install mecab-python3==1.0.4
```

### unidic-lite
日本語の分かち書きで利用する辞書ライブラリの `unidic-lite` をインストールします。
```
pip3 install unidic-lite
```

## 3. 今回取り組む課題についての技術的説明
スライドを使って説明します。スライドはLMSで共有します。

## 4. 今回の実験用データの前処理
今回の実験で利用する文書分類用のデータとして[WRIME: 主観と客観の感情分析データセット](https://github.com/ids-cv/wrime)を利用します。
ライセンスの関係で前処理済みのデータの再配布ができないので、公開されているデータのコピーに対して皆さんのお手元で前処理を実行して利用する必要があります。

### 4.1 前処理スクリプトの作成
まず、[前処理用のPythonスクリプト](https://github.com/ksudoh/lics-exp2-2024/blob/main/11Dec/scripts/extract_wrime_v2_data.py)のコピーを作成します。
以下のいずれかの方法で行ってみてください。
- GitHubの画面を見ながら写経する
- GitHubの画面で Copy raw file のボタンをクリックしてクリップボードにコピーし、何かのエディタで開いて貼り付ける
- 須藤のディレクトリにある `/export/home/ics/sudoh/Project/Exp2/2024/scripts/extract_wrime_v2_data.py` をコピーする

このファイルは `${EXPDIR}/scripts/extract_wrime_v2_data.py` として保存します。

### 4.2 前処理済みデータ格納用ディレクトリの作成
前処理済みのデータを格納するためのディレクトリを作成します。
```
mkdir -p ${EXPDIR}/data
```

### 4.3 前処理スクリプトの実行
以下のように前処理スクリプトを実行し、WRIME v2のデータの必要な箇所だけを抜き取り、JSON形式で格納したファイルを作成します。
```
python3 ${EXPDIR}/scripts/extract_wrime_v2_data.py -t ${EXPDIR}/data/wrime_v2_sentiment.tok.json
```

実行の結果、`${EXPDIR}/data/wrime_v2_sentiment.tok.json` というファイルが生成されるはずです。

また、比較のため分かち書きを行っていない形式のファイルも作成しておきます。
（余力のある人はこのデータを使った実験にもトライしてみてください）
```
python3 ${EXPDIR}/scripts/extract_wrime_v2_data.py ${EXPDIR}/data/wrime_v2_sentiment.json
```

## 5. 文書分類の学習用・推論用プログラムの作成
### 5.1 学習用プログラムの作成
前処理をしたJSON形式のデータを入力とし、学習した文書分類モデルを出力するプログラムを作成してください。
- 入力データファイル名の指定
- 出力モデルファイル名の指定
- 入力データの読み込み
- 入力特徴量の抽出
- scikit-learnのデータ形式への変換
- モデルの学習
- モデルの保存

### 5.2 推論用プログラムの作成
前処理をしたJSON形式のデータと、学習済みの文書分類モデルを入力とし、文書分類の結果を出力するプログラムを作成してください。
- 入力データファイル名の指定
- 入力ファイル名の指定
- 入力データの読み込み
- 入力特徴量の抽出
- scikit-learnのデータ形式への変換
- 分類結果の推論
- 分類結果の保存

## 6. 分類結果の評価用プログラムの作成
前処理をしたJSON形式のデータと、文書分類の結果を入力とし、文書分類の精度を評価して出力するプログラムを作成してください。
- 入力データファイル名の指定
- 入力分類結果ファイル名の指定
- 入力データ・入力分類結果の読み込み
- 評価値の計算
- 評価値の出力表示


## 7. 課題提出（時間内に終わらなければ提出期限までに提出すればOK）

